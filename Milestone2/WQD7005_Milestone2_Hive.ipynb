{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone2: Data Warehouse (HIVE)\n",
    "WQD180102 Ng Wei Xin\n",
    "WQD180104 Tan Bing Shien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from impala.dbapi import connect\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# using requests to fetch pages\n",
    "# Note: Python Selenium library can be used for javascript pages (by imitating browser behaviour)\n",
    "# bs4: Html-Parser (or XML), to navigate through html documents\n",
    "\n",
    "# using impala to connect to HIVE (for Windows)\n",
    "# using HDFS to connect to  Hadoop HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session connections \n",
    "class SessionConnections: \n",
    "    def __init__(self): \n",
    "        \n",
    "        # Connection to HIVE: localhost, default port 10000, default user/password = hive\n",
    "        conn = connect(host='localhost', port=10000, database='datamining', user='hive', password='hive', auth_mechanism=\"PLAIN\")\n",
    "        self.hivecursor = conn.cursor()\n",
    "\n",
    "        # Connecting to Webhdfs: localhost, default port 50070\n",
    "        self.hdfsclient = InsecureClient('http://localhost:50070')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CRAWLING Function\n",
    "\n",
    "## Crawling Source: TheStar Latest News\n",
    "\n",
    "def crawl(objDF):\n",
    "    page = requests.get('https://www.thestar.com.my/news/latest')\n",
    "    contents = page.content\n",
    "\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "    for tag in soup.find_all(class_= \"timeline-content\", limit = 15):\n",
    "        for subtag in tag.find('h2'):\n",
    "            \n",
    "            # News-ID\n",
    "            idNews = \"\".join(line.strip() for line in subtag['data-content-id'].split(\"\\n\"))\n",
    "                        \n",
    "            # Check for duplication, proceed only for new item\n",
    "            # else skip to save computation\n",
    "            if not df['NewsID'].str.contains(idNews).any():\n",
    "                \n",
    "                # Headline\n",
    "                strNews = \"\".join(line.strip() for line in subtag.string.split(\"\\n\"))\n",
    "                \n",
    "                # Category\n",
    "                catNews = \"\".join(line.strip() for line in subtag['data-content-category'].split(\"\\n\"))\n",
    "                \n",
    "                # Timestamp (spider on respective news page)\n",
    "                newsHref = subtag['href']\n",
    "                subPage = requests.get(newsHref)\n",
    "                subContent = subPage.content\n",
    "                subSoup = BeautifulSoup(subContent, 'html.parser')\n",
    "                try:\n",
    "                    dateNews = \"\".join(line.strip() for line in subSoup.find(class_=\"date\").string.split(\"\\n\"))\n",
    "                    dateNews = dateNews.replace(\",\", \" \")\n",
    "                except:\n",
    "                    dateNews = \"-\"\n",
    "                try:\n",
    "                    timeNews = \"\".join(line.strip() for line in subSoup.find(class_=\"timestamp\").string.split(\"\\n\"))\n",
    "                    timeNews = timeNews.replace(\",\", \" \")\n",
    "                except:\n",
    "                    timeNews = \"-\"\n",
    "                # Add info to list\n",
    "                objDF = objDF.append(pd.Series([idNews, dateNews, timeNews, catNews, strNews, newsHref], index=objDF.columns), ignore_index=True)\n",
    "\n",
    "    return objDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty DataFrame:\n",
    "df = pd.DataFrame(columns=[\"NewsID\", \"PublishedDate\", \"PublishedTime\", \"Category\", \"Headline\", \"URL\"])\n",
    "df_prev = df # create temporary df\n",
    "\n",
    "# Test: using i as counter, for recurring crawl.\n",
    "# for endless loop, use while-True loop.\n",
    "i = 2 \n",
    "# while i>0:\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    # Re-Crawl Latest News\n",
    "    df_current = crawl(df_prev)\n",
    "    #print(df)\n",
    "    \n",
    "    # get only unique values:\n",
    "    df = pd.concat([df_current, df_prev])\n",
    "    df = df.drop_duplicates(keep=False, inplace=False)\n",
    "\n",
    "    # Proceed only if there is any new entry:\n",
    "    if df.shape[0] != 0 :\n",
    "        \n",
    "        # save only last 20 crawls in df_prev (buffer for next crawl)\n",
    "        df_prev = df_current.iloc[-20:, :]\n",
    "\n",
    "        # Export dataframe to csv file (save to  LOCAL DIRECTROY)\n",
    "        ## Use current DateTime to store unique csv file \n",
    "        curDateTime = time.strftime(r\"%Y%m%d_%H%M\", time.localtime())\n",
    "        # df.to_csv(r'C:\\Users\\FORGE-15 I7\\OneDrive - AsiaPay Limited\\Sem 3\\WQD7005 DATA MINING\\dataset.csv', index = False)\n",
    "        df.to_csv(r'C:\\Users\\ngwei\\Desktop\\Crawling\\dataset_'+curDateTime+'.csv', index = False)\n",
    "\n",
    "        # Create SESSION (hive / hdfs connection, to avoid disconnection from server)\n",
    "        mySession = SessionConnections()\n",
    "\n",
    "        # Writing Dataframe to hdfs, as csv\n",
    "        with mySession.hdfsclient.write('/user/admin/datamining/tmpcsv.csv', encoding = 'utf-8') as writer:\n",
    "            df.to_csv(writer, index=False)\n",
    "\n",
    "        # load csv data into hive-table\n",
    "        insertSQL = \"LOAD DATA INPATH '/user/admin/datamining/tmpcsv.csv' INTO TABLE mynews\"\n",
    "        mySession.hivecursor.execute(insertSQL)\n",
    "    \n",
    "    # temporary counter-holder\n",
    "    i=i-1\n",
    "    \n",
    "    # Re-Crawl after every x seconds\n",
    "    time.sleep(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ----------------------------------------------------------------\n",
    "# ## WQD7005: Milestone 2, HIVE Data Warehouse\n",
    "# ## Tan Bing Shien WQD180104, Ng Wei Xin WQD180102\n",
    "# ## ----------------------------------------------------------------\n",
    "# ## Writing Data into HIVE\n",
    "# ## Step1: Write DataFrame (windows, local) into hdfs (HDP, VirtualBox)\n",
    "# ## Step2: Load data (csv) into HIVE (database)\n",
    "# ## ----------------------------------------------------------------\n",
    "\n",
    "# # Re-Creating Session to avoid disconnection:\n",
    "# mySession = SessionConnections()\n",
    "\n",
    "# # Writing Dataframe to hdfs\n",
    "# with mySession.hdfsclient.write('/user/admin/datamining/tmpcsv.csv', encoding = 'utf-8') as writer:\n",
    "#     df.to_csv(writer, index=False)\n",
    "\n",
    "# # insert csv data into temporary table\n",
    "# insertSQL = \"LOAD DATA INPATH '/user/admin/datamining/tmpcsv.csv' INTO TABLE mynews\"\n",
    "# mySession.hivecursor.execute(insertSQL)\n",
    "# ## Note: Table created with skip.header.line.count=1 property, would skip first line (header) of csv file.\n",
    "\n",
    "# # verify result\n",
    "# showSQL = \"SELECT * FROM mynews\"\n",
    "# mySession.hivecursor.execute(showSQL)\n",
    "# print(mySession.hivecursor.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# # CREATE TABLE\n",
    "# ------------------------\n",
    "# create mynews table\n",
    "# createSQL = \"CREATE TABLE IF NOT EXISTS mynews \\\n",
    "#             (NewsID STRING, PublishedDate STRING, PublishedTime STRING, Category STRING, Headline STRING, URL STRING) \\\n",
    "#             ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE tblproperties ('skip.header.line.count'='1') \\\n",
    "#             \"\n",
    "# cur.execute(createSQL)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# # SELECT & INSERT DATA\n",
    "# ------------------------\n",
    "\n",
    "# # Select \n",
    "# cur.execute('SELECT * FROM mynews WHERE newsid > 50 LIMIT 1, 10')\n",
    "# print(cur.fetchall())\n",
    "\n",
    "# # Insert \n",
    "# # -- Note: very laggy on HIVE\n",
    "# cur.execute('INSERT INTO testdata (test_id, name, item1, item2) VALUES (52, \"myName\", \"myItem1\", \"myItem2\")')\n",
    "# print(cur.fetchall())\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# ## Build DataFrame \n",
    "# ------------------------\n",
    "# data = pd.read_csv(r\"C:\\Users\\ngwei\\Desktop\\testdata2.csv\")\n",
    "# print(data)\n",
    "\n",
    "# ------------------------\n",
    "# ## Create Connections to HIVE / HDFS \n",
    "# ------------------------\n",
    "# # Connection to HIVE: localhost, default port 10000, default user/password = hive\n",
    "# conn = connect(host='localhost', port=10000, database='datamining', user='hive', password='hive', auth_mechanism=\"PLAIN\")\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Connecting to Webhdfs: localhost, default port 50070\n",
    "# client_hdfs = InsecureClient('http://localhost:50070')\n",
    "# ------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
